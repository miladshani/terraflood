{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 - Model 1 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, UpSampling2D, concatenate, LeakyReLU, ReLU\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.metrics import MeanIoU\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tifffile\n",
    "from pathlib import Path\n",
    "import os\n",
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Project path\n",
    "TERRAFLOOD = Path('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------ inputs --------------\n",
    "# experiment's meta data\n",
    "experiment_number = 1\n",
    "model_architecture = 1\n",
    "epochs = 1\n",
    "early_stopping_patience = 10\n",
    "learning_rate_patience = 5\n",
    "\n",
    "# input and output paths\n",
    "load_path = TERRAFLOOD.joinpath(\"data/dataset_balanced/\")\n",
    "save_path = TERRAFLOOD.joinpath(f\"experiments/model_{model_architecture}_exp_{experiment_number}/\")\n",
    "\n",
    "# structure of logging and saving with naming convention\n",
    "# Checkpoint of the model\n",
    "checkpoint_dir = save_path.joinpath(\"checkpoint/\")\n",
    "checkpoint_path = checkpoint_dir / f\"model_{model_architecture}_check_exp_{experiment_number}_epochs_{epochs}_patience_{early_stopping_patience}_{learning_rate_patience}.keras\"\n",
    "\n",
    "# Logging on tensorboard\n",
    "tensorboard_logs_dir = save_path.joinpath(\"tensorboard_log/\")\n",
    "\n",
    "# Saving final log on ML-Flow\n",
    "mlflow_logs_dir = save_path.joinpath(\"mlflow_log/\")\n",
    "\n",
    "# Saving the final model\n",
    "model_save_dir = save_path.joinpath(\"model/\")\n",
    "model_path = model_save_dir / f\"model_{model_architecture}_exp_{experiment_number}_epochs_{epochs}_patience_{early_stopping_patience}_{learning_rate_patience}.keras\"\n",
    "\n",
    "# directory existence ensurance\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "tensorboard_logs_dir.mkdir(parents=True, exist_ok=True)\n",
    "mlflow_logs_dir.mkdir(parents=True, exist_ok=True)\n",
    "model_save_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.17.0\n",
      "Available devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Mon Aug 12 14:10:19 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla P100-PCIE-16GB           On  | 00000000:01:00.0 Off |                    0 |\n",
      "| N/A   30C    P0              31W / 250W |  15812MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Hardware info\n",
    "# Print TensorFlow version\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# List available devices\n",
    "devices = tf.config.list_physical_devices()\n",
    "print(\"Available devices:\", devices)\n",
    "\n",
    "# GPU info in details (assuming nvidia as GPU device)\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Jaccard Loss\n",
    "def jaccard_loss(y_true, y_pred, smooth=100):\n",
    "    \"\"\"\n",
    "    Calculates the Jaccard loss, also known as the Intersection over Union (IoU) loss.\n",
    "    Args:\n",
    "        y_true (tensor): Ground truth labels.\n",
    "        y_pred (tensor): Predicted labels.\n",
    "        smooth (float): Smoothing factor to avoid division by zero.\n",
    "    Returns:\n",
    "        jaccard loss (tensor)\n",
    "    \"\"\"\n",
    "    y_true_f = tf.keras.backend.flatten(y_true)\n",
    "    y_pred_f = tf.keras.backend.flatten(y_pred)\n",
    "    intersection = tf.keras.backend.sum(y_true_f * y_pred_f)\n",
    "    sum_ = tf.keras.backend.sum(y_true_f + y_pred_f)\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "    return 1 - jac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define U-Net model (single channel)\n",
    "def unet_one_channel(input_size=(256, 256, 1), loss_function='jaccard_loss'):\n",
    "    inputs = Input(input_size)\n",
    "\n",
    "    def conv_block(inputs, num_filters):\n",
    "        conv = Conv2D(num_filters, 3, activation=LeakyReLU(), padding=\"same\")(inputs)\n",
    "        conv = Conv2D(num_filters, 3, activation=LeakyReLU(), padding=\"same\")(conv)\n",
    "        return conv\n",
    "\n",
    "    conv1 = conv_block(inputs, 64)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = conv_block(pool1, 128)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = conv_block(pool2, 256)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = conv_block(pool3, 512)\n",
    "    drop4 = Dropout(0.3)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "    conv5 = conv_block(pool4, 1024)\n",
    "    drop5 = Dropout(0.3)(conv5)\n",
    "\n",
    "    def up_block(inputs, skip_connection, num_filters):\n",
    "        up = Conv2D(num_filters, 2, activation=LeakyReLU(), padding=\"same\")(UpSampling2D(size=(2, 2))(inputs))\n",
    "        merge = concatenate([skip_connection, up], axis=3)\n",
    "        conv = conv_block(merge, num_filters)\n",
    "        return conv\n",
    "\n",
    "    conv6 = up_block(drop5, drop4, 512)\n",
    "    conv7 = up_block(conv6, conv3, 256)\n",
    "    conv8 = up_block(conv7, conv2, 128)\n",
    "    conv9 = up_block(conv8, conv1, 64)\n",
    "\n",
    "    conv9 = Conv2D(2, 1, activation=LeakyReLU(), padding=\"same\")(conv9)\n",
    "    conv10 = Conv2D(1, 1, activation=\"sigmoid\")(conv9)\n",
    "\n",
    "    model = Model(inputs, conv10)\n",
    "\n",
    "    model.compile(optimizer=RMSprop(learning_rate=1e-4), loss=jaccard_loss, metrics=[\"accuracy\", MeanIoU(num_classes=2)])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the data with progress bar\n",
    "# (in case of enough ram)\n",
    "def load_and_preprocess_data(image_paths, mask_paths, image_size=(256, 256)):\n",
    "    X = np.empty((len(image_paths), *image_size, 1), dtype=np.float32)\n",
    "    y = np.empty((len(mask_paths), *image_size, 1), dtype=np.float32)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    with tqdm(total=len(image_paths), desc=\"Loading and preprocessing data\") as pbar:\n",
    "        for i, (image_path, mask_path) in enumerate(zip(image_paths, mask_paths)):\n",
    "            img = tifffile.imread(image_path)\n",
    "            img = np.expand_dims(img, axis=-1) if img.ndim == 2 else img\n",
    "            img = (img - (-25)) / (5 - (-25))  # Normalize to [0, 1]\n",
    "            X[i] = img\n",
    "\n",
    "            mask = tifffile.imread(mask_path)\n",
    "            mask = np.expand_dims(mask, axis=-1) if mask.ndim == 2 else mask\n",
    "            y[i] = mask\n",
    "\n",
    "            elapsed_time = time.time() - start_time\n",
    "            estimated_total_time = elapsed_time / (i + 1) * len(image_paths)\n",
    "            remaining_time = estimated_total_time - elapsed_time\n",
    "            pbar.set_postfix({\n",
    "                'Elapsed': f\"{elapsed_time:.2f}s\",\n",
    "                'ETA': f\"{remaining_time:.2f}s\"\n",
    "            })\n",
    "            pbar.update(1)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "# DataLoader (in case of huge dataset)\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, image_paths, mask_paths, batch_size=32, image_size=(256, 256), n_channels=1, shuffle=True):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.n_channels = n_channels\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.image_paths) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index * self.batch_size : (index + 1) * self.batch_size]\n",
    "        image_paths_batch = [self.image_paths[i] for i in indexes]\n",
    "        mask_paths_batch = [self.mask_paths[i] for i in indexes]\n",
    "        \n",
    "        X, y = self.__data_generation(image_paths_batch, mask_paths_batch)\n",
    "        \n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.image_paths))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, image_paths_batch, mask_paths_batch):\n",
    "        X = np.empty((len(image_paths_batch), *self.image_size, self.n_channels))\n",
    "        y = np.empty((len(mask_paths_batch), *self.image_size, 1))\n",
    "\n",
    "        for i, (image_path, mask_path) in enumerate(zip(image_paths_batch, mask_paths_batch)):\n",
    "            # Load image\n",
    "            img = tifffile.imread(image_path)\n",
    "            img = np.expand_dims(img, axis=-1) if self.n_channels == 1 else img\n",
    "            img = img.astype(np.float32)\n",
    "\n",
    "            img = (img - (-25)) / (5 - (-25))  # Normalize to [0, 1] \n",
    "            img = np.squeeze(img)  # Remove singleton dimension if present\n",
    "            # img = cv2.resize(img, self.image_size)\n",
    "            X[i,] = img\n",
    "\n",
    "            # Load mask\n",
    "            mask = tifffile.imread(mask_path)\n",
    "            mask = np.expand_dims(mask, axis=-1)\n",
    "            mask = mask.astype(np.float32)\n",
    "\n",
    "            y[i,] = mask\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Loggings\n",
    "# Define the MetricsLogger class\n",
    "class MetricsLogger(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is not None:\n",
    "            # Log metrics to MLflow\n",
    "            mlflow.log_metrics({\n",
    "                'loss': logs.get('loss'),\n",
    "                'accuracy': logs.get('accuracy'),\n",
    "                'val_loss': logs.get('val_loss'),\n",
    "                'val_accuracy': logs.get('val_accuracy')\n",
    "            }, step=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3309\n",
      "3309\n"
     ]
    }
   ],
   "source": [
    "# Loading data vvvvvvvv\n",
    "# Reading the paths of all files\n",
    "scene_dirs = [d for d in load_path.iterdir() if d.is_dir()] # and d.name != \"scene1\"] # Uncomment to have one scene totally untouched\n",
    "\n",
    "X_path = []\n",
    "y_path = []\n",
    "\n",
    "for scene_dir in scene_dirs:\n",
    "    data_list = os.listdir(scene_dir)\n",
    "    X_path.extend([scene_dir / f\"{x}/vv.tif\" for x in data_list])\n",
    "    y_path.extend([scene_dir / f\"{x}/mask.tif\" for x in data_list])\n",
    "\n",
    "print(len(X_path), len(y_path), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: 2647, y_train: 2647, X_test: 662, y_test: 662\n"
     ]
    }
   ],
   "source": [
    "# Train test split\n",
    "X_train_paths, X_test_paths, y_train_paths, y_test_paths = train_test_split(X_path, y_path, test_size=0.2, shuffle=False)\n",
    "print(f\"X_train: {len(X_train_paths)}, y_train: {len(y_train_paths)}, X_test: {len(X_test_paths)}, y_test: {len(y_test_paths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2647/2647 [00:06<00:00, 426.59it/s, Elapsed=6.21s, ETA=0.00s]\n",
      "Loading and preprocessing data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 662/662 [00:01<00:00, 363.47it/s, Elapsed=1.82s, ETA=-0.00s]\n"
     ]
    }
   ],
   "source": [
    "# Loading files into RAM (assuming having enough RAM space, not using DataGenerator here)\n",
    "\n",
    "# Load and preprocess the data with progress bar\n",
    "X_train, y_train = load_and_preprocess_data(X_train_paths, y_train_paths)\n",
    "X_test, y_test = load_and_preprocess_data(X_test_paths, y_test_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrain preprations  vvvvv\n",
    "\n",
    "# Model Checkpointing\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(str(checkpoint_path), monitor='val_loss', save_best_only=True, mode='min')\n",
    "\n",
    "# Early Stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=early_stopping_patience, restore_best_weights=True)\n",
    "\n",
    "# TensorBoard Callback\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=str(tensorboard_logs_dir), histogram_freq=1)\n",
    "\n",
    "# Learning Rate Reducer\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=learning_rate_patience, min_lr=1e-6)\n",
    "\n",
    "# For loading model from checkpoint\n",
    "custom_objects = {\n",
    "    'jaccard_loss': jaccard_loss,\n",
    "    'MeanIoU': MeanIoU(num_classes=2),\n",
    "    'LeakyReLU': LeakyReLU,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint found. Loading model from checkpoint...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m166/166\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 460ms/step - accuracy: 0.9262 - loss: 0.1263 - mean_io_u_1: 0.5621"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-12 14:30:06.832043: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 2264924160 exceeds 10% of free system memory.\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=4, connect=5, read=4, redirect=5, status=5)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /api/2.0/mlflow-artifacts/artifacts/0/05c3d6f6da594399a0d34d13ae3e4655/artifacts/checkpoints/latest_checkpoint.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m166/166\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 781ms/step - accuracy: 0.9262 - loss: 0.1263 - mean_io_u_1: 0.5616 - val_accuracy: 0.8684 - val_loss: 0.1254 - val_mean_io_u_1: 0.7660 - learning_rate: 1.0000e-04\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 772ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/12 14:30:53 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmpnfosqmhy/model, flavor: tensorflow). Fall back to return ['tensorflow==2.17.0', 'cloudpickle==3.0.0']. Set logging level to DEBUG to see the full traceback. \n",
      "2024/08/12 14:31:10 WARNING mlflow.keras.save: You are saving a Keras model without specifying model signature.\n",
      "2024/08/12 14:31:18 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n",
      "2024/08/12 14:31:19 INFO mlflow.tracking._tracking_service.client: üèÉ View run monumental-newt-844 at: http://ml-flow-dev-mlflow:5000/#/experiments/0/runs/05c3d6f6da594399a0d34d13ae3e4655.\n",
      "2024/08/12 14:31:19 INFO mlflow.tracking._tracking_service.client: üß™ View experiment at: http://ml-flow-dev-mlflow:5000/#/experiments/0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logged and saved successfully!\n",
      "../experiments/model_1_exp_1/mlflow_log\n",
      "../experiments/model_1_exp_1/model/model_1_exp_1_epochs_1_patience_10_5.keras\n"
     ]
    }
   ],
   "source": [
    "# ------------ Training the model ---------------\n",
    "\n",
    "# Initialize MLflow\n",
    "mlflow.tensorflow.autolog()\n",
    "\n",
    "# Start MLflow experiment\n",
    "with mlflow.start_run() as run:\n",
    "    # Check if checkpoint exists\n",
    "    if checkpoint_path.exists():\n",
    "        print(\"Checkpoint found. Loading model from checkpoint...\")\n",
    "        model = tf.keras.models.load_model(str(checkpoint_path), custom_objects=custom_objects)\n",
    "    else:\n",
    "        print(\"No checkpoint found. Initializing new model...\")\n",
    "        model = unet_one_channel(input_size=(256, 256, 1), loss_function='jaccard_loss')\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=16,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[checkpoint, early_stopping, tensorboard_callback, MetricsLogger(), reduce_lr]\n",
    "    )\n",
    "\n",
    "    # Define conda environment\n",
    "    # For compatibility with ml-flow fixed on the pod\n",
    "    conda_env = {\n",
    "        'channels': ['defaults'],\n",
    "        'dependencies': [\n",
    "            'python=3.11',\n",
    "            'tensorflow=2.17.0',\n",
    "            'keras=3.3.3',\n",
    "            'cloudpickle=3.0.0',\n",
    "            'pip',\n",
    "            {\n",
    "                'pip': ['mlflow', 'cloudpickle==3.0.0']\n",
    "            }\n",
    "        ],\n",
    "        'name': 'mlflow-env'\n",
    "    }\n",
    "\n",
    "    # Log the final model to MLflow\n",
    "    mlflow.keras.log_model(model, \"model\", conda_env=conda_env)  # Use \"model\" as a valid path\n",
    "\n",
    "    # Save model after training ends\n",
    "    model.save(str(model_path) + '.keras')  # Save in Keras format\n",
    "\n",
    "    print(\"Model logged and saved successfully!\")\n",
    "    print(mlflow_logs_dir, model_path, sep=\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "terraflood",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
