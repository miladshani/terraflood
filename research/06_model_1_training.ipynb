{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 - Model 1 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, UpSampling2D, concatenate, LeakyReLU, ReLU\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.metrics import MeanIoU\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tifffile\n",
    "from pathlib import Path\n",
    "import os\n",
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Project path\n",
    "TERRAFLOOD = Path('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------ inputs --------------\n",
    "# experiment's meta data\n",
    "experiment_number = 1\n",
    "model_architectre = 1\n",
    "epochs = 200\n",
    "early_stopping_patience = 10\n",
    "learning_rate_patience = 5\n",
    "\n",
    "# input and output paths\n",
    "load_path = TERRAFLOOD.joinpath(\"dataset_balanced/\")\n",
    "save_path = TERRAFLOOD.joinpath(f\"experiments/model_{model_architecture}_exp_{experiment_number}/\")\n",
    "\n",
    "# structure of logging and saving with naming convention\n",
    "# Checkpoint of the model\n",
    "checkpoint_dir = save_path.joinpath(\"checkpoint/\")\n",
    "checkpoint_path = checkpoint_dir / f\"model_{model_architecture}_check_exp_{experiment_number}_epochs_{epochs}_patience_{early_stopping_patience}_{learning_rate_patience}.keras\"\n",
    "\n",
    "# Logging on tensorboard\n",
    "tensorboard_logs_dir = save_path.joinpath(\"tensorboard_log/\")\n",
    "\n",
    "# Saving final log on ML-Flow\n",
    "mlflow_logs_dir = save_path.joinpath(\"mlflow_log/\")\n",
    "\n",
    "# Saving the final model\n",
    "model_save_dir = save_path.joinpath(\"model/\")\n",
    "model_path = model_save_dir / f\"model_{model_architecture}_exp_{experiment_number}_epochs_{epochs}_patience_{early_stopping_patience}_{learning_rate_patience}.keras\"\n",
    "\n",
    "# directory existence ensurance\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "tensorboard_logs_dir.mkdir(parents=True, exist_ok=True)\n",
    "mlflow_logs_dir.mkdir(parents=True, exist_ok=True)\n",
    "model_save_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hardware info\n",
    "# Print TensorFlow version\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# List available devices\n",
    "devices = tf.config.list_physical_devices()\n",
    "print(\"Available devices:\", devices)\n",
    "\n",
    "# GPU info in details (assuming nvidia as GPU device)\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Jaccard Loss\n",
    "def jaccard_loss(y_true, y_pred, smooth=100):\n",
    "    \"\"\"\n",
    "    Calculates the Jaccard loss, also known as the Intersection over Union (IoU) loss.\n",
    "    Args:\n",
    "        y_true (tensor): Ground truth labels.\n",
    "        y_pred (tensor): Predicted labels.\n",
    "        smooth (float): Smoothing factor to avoid division by zero.\n",
    "    Returns:\n",
    "        jaccard loss (tensor)\n",
    "    \"\"\"\n",
    "    y_true_f = tf.keras.backend.flatten(y_true)\n",
    "    y_pred_f = tf.keras.backend.flatten(y_pred)\n",
    "    intersection = tf.keras.backend.sum(y_true_f * y_pred_f)\n",
    "    sum_ = tf.keras.backend.sum(y_true_f + y_pred_f)\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "    return 1 - jac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define U-Net model (single channel)\n",
    "def unet_one_channel(input_size=(256, 256, 1), loss_function='jaccard_loss'):\n",
    "    inputs = Input(input_size)\n",
    "\n",
    "    def conv_block(inputs, num_filters):\n",
    "        conv = Conv2D(num_filters, 3, activation=LeakyReLU(), padding=\"same\")(inputs)\n",
    "        conv = Conv2D(num_filters, 3, activation=LeakyReLU(), padding=\"same\")(conv)\n",
    "        return conv\n",
    "\n",
    "    conv1 = conv_block(inputs, 64)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = conv_block(pool1, 128)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = conv_block(pool2, 256)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = conv_block(pool3, 512)\n",
    "    drop4 = Dropout(0.3)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "    conv5 = conv_block(pool4, 1024)\n",
    "    drop5 = Dropout(0.3)(conv5)\n",
    "\n",
    "    def up_block(inputs, skip_connection, num_filters):\n",
    "        up = Conv2D(num_filters, 2, activation=LeakyReLU(), padding=\"same\")(UpSampling2D(size=(2, 2))(inputs))\n",
    "        merge = concatenate([skip_connection, up], axis=3)\n",
    "        conv = conv_block(merge, num_filters)\n",
    "        return conv\n",
    "\n",
    "    conv6 = up_block(drop5, drop4, 512)\n",
    "    conv7 = up_block(conv6, conv3, 256)\n",
    "    conv8 = up_block(conv7, conv2, 128)\n",
    "    conv9 = up_block(conv8, conv1, 64)\n",
    "\n",
    "    conv9 = Conv2D(2, 1, activation=LeakyReLU(), padding=\"same\")(conv9)\n",
    "    conv10 = Conv2D(1, 1, activation=\"sigmoid\")(conv9)\n",
    "\n",
    "    model = Model(inputs, conv10)\n",
    "\n",
    "    model.compile(optimizer=RMSprop(learning_rate=1e-4), loss=jaccard_loss, metrics=[\"accuracy\", MeanIoU(num_classes=2)])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the data with progress bar\n",
    "# (in case of enough ram)\n",
    "def load_and_preprocess_data(image_paths, mask_paths, image_size=(256, 256)):\n",
    "    X = np.empty((len(image_paths), *image_size, 1), dtype=np.float32)\n",
    "    y = np.empty((len(mask_paths), *image_size, 1), dtype=np.float32)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    with tqdm(total=len(image_paths), desc=\"Loading and preprocessing data\") as pbar:\n",
    "        for i, (image_path, mask_path) in enumerate(zip(image_paths, mask_paths)):\n",
    "            img = tifffile.imread(image_path)\n",
    "            img = np.expand_dims(img, axis=-1) if img.ndim == 2 else img\n",
    "            img = (img - (-25)) / (5 - (-25))  # Normalize to [0, 1]\n",
    "            X[i] = img\n",
    "\n",
    "            mask = tifffile.imread(mask_path)\n",
    "            mask = np.expand_dims(mask, axis=-1) if mask.ndim == 2 else mask\n",
    "            y[i] = mask\n",
    "\n",
    "            elapsed_time = time.time() - start_time\n",
    "            estimated_total_time = elapsed_time / (i + 1) * len(image_paths)\n",
    "            remaining_time = estimated_total_time - elapsed_time\n",
    "            pbar.set_postfix({\n",
    "                'Elapsed': f\"{elapsed_time:.2f}s\",\n",
    "                'ETA': f\"{remaining_time:.2f}s\"\n",
    "            })\n",
    "            pbar.update(1)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "# DataLoader (in case of huge dataset)\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, image_paths, mask_paths, batch_size=32, image_size=(256, 256), n_channels=1, shuffle=True):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.n_channels = n_channels\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.image_paths) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index * self.batch_size : (index + 1) * self.batch_size]\n",
    "        image_paths_batch = [self.image_paths[i] for i in indexes]\n",
    "        mask_paths_batch = [self.mask_paths[i] for i in indexes]\n",
    "        \n",
    "        X, y = self.__data_generation(image_paths_batch, mask_paths_batch)\n",
    "        \n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.image_paths))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, image_paths_batch, mask_paths_batch):\n",
    "        X = np.empty((len(image_paths_batch), *self.image_size, self.n_channels))\n",
    "        y = np.empty((len(mask_paths_batch), *self.image_size, 1))\n",
    "\n",
    "        for i, (image_path, mask_path) in enumerate(zip(image_paths_batch, mask_paths_batch)):\n",
    "            # Load image\n",
    "            img = tifffile.imread(image_path)\n",
    "            img = np.expand_dims(img, axis=-1) if self.n_channels == 1 else img\n",
    "            img = img.astype(np.float32)\n",
    "\n",
    "            img = (img - (-25)) / (5 - (-25))  # Normalize to [0, 1] \n",
    "            img = np.squeeze(img)  # Remove singleton dimension if present\n",
    "            # img = cv2.resize(img, self.image_size)\n",
    "            X[i,] = img\n",
    "\n",
    "            # Load mask\n",
    "            mask = tifffile.imread(mask_path)\n",
    "            mask = np.expand_dims(mask, axis=-1)\n",
    "            mask = mask.astype(np.float32)\n",
    "\n",
    "            y[i,] = mask\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Loggings\n",
    "# Define the MetricsLogger class\n",
    "class MetricsLogger(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is not None:\n",
    "            # Log metrics to MLflow\n",
    "            mlflow.log_metrics({\n",
    "                'loss': logs.get('loss'),\n",
    "                'accuracy': logs.get('accuracy'),\n",
    "                'val_loss': logs.get('val_loss'),\n",
    "                'val_accuracy': logs.get('val_accuracy')\n",
    "            }, step=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data vvvvvvvv\n",
    "# Reading the paths of all files\n",
    "scene_dirs = [d for d in load_path.iterdir() if d.is_dir()] # and d.name != \"scene1\"] # Uncomment to have one scene totally untouched\n",
    "\n",
    "X_path = []\n",
    "y_path = []\n",
    "\n",
    "for scene_dir in scene_dirs:\n",
    "    data_list = os.listdir(scene_dir)\n",
    "    X_path.extend([scene_dir / f\"{x}/vv.tif\" for x in data_list])\n",
    "    y_path.extend([scene_dir / f\"{x}/mask.tif\" for x in data_list])\n",
    "\n",
    "print(len(X_path), len(y_path), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "X_train_paths, X_test_paths, y_train_paths, y_test_paths = train_test_split(X_path, y_path, test_size=0.2, shuffle=False)\n",
    "print(f\"X_train: {len(X_train_paths)}, y_train: {len(y_train_paths)}, X_test: {len(X_test_paths)}, y_test: {len(y_test_paths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading files into RAM (assuming having enough RAM space, not using DataGenerator here)\n",
    "\n",
    "# Load and preprocess the data with progress bar\n",
    "X_train, y_train = load_and_preprocess_data(X_train_paths, y_train_paths)\n",
    "X_test, y_test = load_and_preprocess_data(X_test_paths, y_test_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrain preprations  vvvvv\n",
    "\n",
    "# Model Checkpointing\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(str(checkpoint_path), monitor='val_loss', save_best_only=True, mode='min')\n",
    "\n",
    "# Early Stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=early_stopping_pacience, restore_best_weights=True)\n",
    "\n",
    "# TensorBoard Callback\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=str(tensorboard_logs_dir), histogram_freq=1)\n",
    "\n",
    "# Learning Rate Reducer\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=learning_rate_patience, min_lr=1e-6)\n",
    "\n",
    "# For loading model from checkpoint\n",
    "custom_objects = {\n",
    "    'jaccard_loss': jaccard_loss,\n",
    "    'MeanIoU': MeanIoU(num_classes=2),\n",
    "    'LeakyReLU': LeakyReLU,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------ Training the model ---------------\n",
    "\n",
    "# Initialize MLflow\n",
    "mlflow.tensorflow.autolog()\n",
    "\n",
    "# Start MLflow experiment\n",
    "with mlflow.start_run() as run:\n",
    "    # Check if checkpoint exists\n",
    "    if checkpoint_path.exists():\n",
    "        print(\"Checkpoint found. Loading model from checkpoint...\")\n",
    "        model = tf.keras.models.load_model(str(checkpoint_path), custom_objects=custom_objects)\n",
    "    else:\n",
    "        print(\"No checkpoint found. Initializing new model...\")\n",
    "        model = unet_one_channel(input_size=(256, 256, 1), loss_function='jaccard_loss')\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=16,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[checkpoint, early_stopping, tensorboard_callback, MetricsLogger(), reduce_lr]\n",
    "    )\n",
    "\n",
    "    # Log the final model to MLflow\n",
    "    mlflow.keras.log_model(model, mlflow_logs_dir)\n",
    "\n",
    "    # Save model after training ends\n",
    "    model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
